{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opr6ig7eC7LT",
    "outputId": "152cc5f4-6211-4a27-c379-a51fd0683856"
   },
   "outputs": [],
   "source": [
    "# === MOUNT GOOGLE DRIVE ===\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# === STANDARD IMPORTS ===\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# === CREATE /content/sample_data IF NOT EXIST ===\n",
    "os.makedirs(\"/content/sample_data\", exist_ok=True)\n",
    "EXTRACT_BASE = \"/content/sample_data\"\n",
    "\n",
    "# === UNZIP DATASETS ===\n",
    "ZIP_PATHS = {\n",
    "    \"flickr8k\": \"/content/drive/MyDrive/Flickr8k.zip\",\n",
    "    \"flickr30k\": \"/content/drive/MyDrive/Flickr30k.zip\"\n",
    "}\n",
    "\n",
    "for name, zip_path in ZIP_PATHS.items():\n",
    "    extract_path = os.path.join(EXTRACT_BASE, name)\n",
    "    if not os.path.exists(os.path.join(extract_path, \"Images\")):  # Skip if already extracted\n",
    "        print(f\"📦 Extracting {name} dataset...\")\n",
    "        os.makedirs(extract_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "    else:\n",
    "        print(f\"✅ {name} dataset already extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coMgv_40C7LT",
    "outputId": "ff8b716a-0f9e-4045-e48b-98c5f67d5912"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers torchvision scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7b2bb1f5251b4a4a8edd51d50d4d4811",
      "fe9df763447947d29c19da836ab72226",
      "c778268193fe4e82a840f6afea3dc373",
      "9fc2fda379384ee3a648e089e63ad6df",
      "21fcda732c80476197129a6a015e0208",
      "54d7c4fdb2904b8fa8fa1afa92c7c475",
      "5804cccc07734bc8b8bc50edd2868af6",
      "09fa247fdb3a418db042e2ae226147d5",
      "5dd8c13fa7614d2583d52e1a70c75e27",
      "842e369c1fdb4980aa01c04ad0824b15",
      "514d733028d64a328d6e7cbb6c42bfcb",
      "7344ed4f3e5f4f88883a29fc2203096e",
      "e2a50bbb95d4473fa87b66f9a28472eb",
      "2b5506884f754cfb94009611ebd6d5ec",
      "1c2c1d5ac5734709bd532dc9e2d1a350",
      "ad847e186361455085da6823e20ca14c",
      "ef1bbf83e1cb4e9cb98a44929423b59e",
      "2760f75994994493ae5282a192622785",
      "7e9a9638447148ad95ca05985392f076",
      "73125ec1d6154026b3f1385b13a42419",
      "141bb96101eb491aa61f3daa991f2db2",
      "68ea849b8a614244afbe3dc33dccdfe1",
      "6d3219dcde8546bebfdc45ffd9cb97b8",
      "b771325bc7f942d7b8d7300eface6f98",
      "fba245e9cc68465b85e2269c08e39e4e",
      "140f9b8b60d045bf84eab0ad597c7697",
      "08738229f14442538363ea265804650a",
      "269d557cba96439187dc4b603bb93316",
      "cadfcba7ecfa463498de1ee3c8dd8da4",
      "5c7808fd68c944519e30b8e0cbc4c0c0",
      "6aa74859bb674acebdd92d3e019cf717",
      "1296be9fc56840ed870e5328a743a58d",
      "6a778b0bd9ed45989637c820e724d333",
      "460ee5c5819648ea991d10f45f02c69f",
      "644ba76b9fb24917af97183ee9abb414",
      "3a4eb186bae247669b92129c5464e768",
      "af18a02016354f99b7914526f77d5ff6",
      "fdc97dad0f7f4faaa3a765862588043f",
      "6a760317028041a8a632fd151576560a",
      "55da7c922df74f229374544699a20199",
      "6cc64f3363dd439bb2a6b53869a9ef0e",
      "ff6911d4d69d48b6bd148175070b8c77",
      "8160e5aaaf1d4552b26f3af788cb4e7c",
      "648c91c422d5470e9f0f487f8bd328be",
      "3bdf5b83fade428695f026868c4cb17a",
      "19dffcf052bf449cbb3ba653ad08d191",
      "78d797c2044947ba9883f309eb293893",
      "a845013d41084902adccedb06c522542",
      "6d51641e8a524407a3d34feb1c9b4348",
      "ed430b1bdeca460cba862a880d5570b3",
      "483a631af65645478a76dc4d6e0eb46a",
      "aadaca9b70f440cda397ca9e9c66e52a",
      "cc114cf79d4f4e5da9f8790d1e888ae4",
      "faaffbe30192499d999414f09c4e5567",
      "33574e2eebc84a1ab87a81642d721091",
      "16159dfde3294b0a8eb8ce705153a1ff",
      "38d334a5050c4d5f99f2a67df89929e5",
      "ec7d0990b8c04028803809684bc3e410",
      "eaa11541259b4bada9bcefe6347c4b4f",
      "2046f0e9169d4184bb7db8c889ae6e67",
      "4e1e29db0e894145bee23994a81db96e",
      "66dbc9e5fcb443879ca627198c24c4d0",
      "e2bf3806d6ec41feb6a864ab5f2eaec5",
      "7912ca9f12a34693b793dd589d9ce1da",
      "0bab4ac3e33747d6ba05217752b3d31e",
      "ec25a4410e5940b29bb3222aaf2fd13f",
      "5c3ff3439aba44b6829f4bb49f91523a",
      "f4d47d793cbd480187c3192721f3af1d",
      "99c3086d7f55461d8d36c15c8b417317",
      "16a36f9d2214484f913ecfc6e04053af",
      "d09fb6e0e0584fa284dc8de8c87187b7",
      "e41dd8fab26c4d95ac7f404b96af1e46",
      "0f0ada556fdb4044af74e9c4e596f346",
      "39b23c230358403d9cfd7e55880c2f08",
      "4e27d9b14e35420ab20f03c839b7bb66",
      "f1febd3edc114944bd967cc2c1814e06",
      "4f6c9b43431c4b65b959fa4c348584e9",
      "2380aca61b384add900bbfbbacc9fe29",
      "1b55603063be429ba20ffe8e06d05390",
      "5039ce1192a547de86a36f71697c7e0d",
      "2eb63153c97143d5baad39b5b0b63956",
      "345de667db3241c2a7f414ff524b3985",
      "4c5be64f66f44bf189bd0056f51f5524",
      "db0e7b2cad8e4e1eb8f2ee7a5b165637",
      "d072082fcc304ab1992281b2cdd5c820",
      "7aafe74c896b4e5f83dfeff7e6e4ee8f",
      "8cb7d57346e44605a53779c0cfde2868",
      "42b57c03881c4003810d860251ca4d3b",
      "6820abe205cf45d5add85eb5c226df94",
      "654b8c463bd24e0a85c242677e2c913d",
      "59b9b2b057974886af66195e243a90d1",
      "4baf3581cc2043c99e507d9fc720be57",
      "bce8b50c982948728eba9dd5065e70df",
      "9fff25bc9dc54319985cd397b09a30ca",
      "2d9fd83489c64b11bf662a66ac143c76",
      "a3dfd268543b4621af306c93de81ce84",
      "ec7344ff24f248aba9d945d2ec14b186",
      "8a975935dfa747b08dd1e9f31da22bae",
      "eab895fc8fd2468488356e752877c66d"
     ]
    },
    "id": "rD-XmPUTC7LT",
    "outputId": "52e34b3a-afd5-4283-edce-4bb3ed752df0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjrFMdNGC7LT"
   },
   "outputs": [],
   "source": [
    "# === HELPER FUNCTIONS ===\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "TOP_K_VALUES = [1, 3, 5]\n",
    "\n",
    "def load_captions(filepath):\n",
    "    mapping = defaultdict(list)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',', 1)\n",
    "            if len(parts) == 2:\n",
    "                image_id, caption = parts\n",
    "                mapping[image_id.strip()].append(caption.strip())\n",
    "    return mapping\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vec = model.get_image_features(**inputs).squeeze().cpu()\n",
    "    return vec\n",
    "\n",
    "def process_caption(caption):\n",
    "    inputs = processor(text=[caption], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vec = model.get_text_features(**inputs).squeeze().cpu()\n",
    "    return vec\n",
    "\n",
    "def generate_embeddings(image_dir, caption_file, embedding_dir):\n",
    "    os.makedirs(embedding_dir, exist_ok=True)\n",
    "    img_to_caps = load_captions(caption_file)\n",
    "\n",
    "    for img_file in tqdm(sorted(img_to_caps), desc=f\"Embedding {os.path.basename(embedding_dir)}\"):\n",
    "        img_name = os.path.splitext(img_file)[0]\n",
    "        img_out_path = os.path.join(embedding_dir, f\"{img_name}.pt\")\n",
    "        cap_out_paths = [os.path.join(embedding_dir, f\"{img_name}_cap_{i+1}.pt\") for i in range(5)]\n",
    "\n",
    "        if os.path.exists(img_out_path) and all(os.path.exists(p) for p in cap_out_paths):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img_vec = process_image(os.path.join(image_dir, img_file))\n",
    "            torch.save(img_vec, img_out_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping image {img_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for i, cap in enumerate(img_to_caps[img_file][:5]):\n",
    "            try:\n",
    "                cap_vec = process_caption(cap)\n",
    "                torch.save(cap_vec, cap_out_paths[i])\n",
    "            except Exception as e:\n",
    "                print(f\"Caption {i+1} for {img_file} failed: {e}\")\n",
    "\n",
    "def load_embeddings(path):\n",
    "    img_emb, cap_emb, cap2img = {}, {}, defaultdict(list)\n",
    "    for fname in os.listdir(path):\n",
    "        if not fname.endswith(\".pt\"):\n",
    "            continue\n",
    "        base = fname[:-3]\n",
    "        full_path = os.path.join(path, fname)\n",
    "        vec = torch.load(full_path)\n",
    "        if \"_cap_\" in base:\n",
    "            cap_emb[base] = vec\n",
    "            img_id = base.split(\"_cap_\")[0]\n",
    "            cap2img[img_id].append(base)\n",
    "        else:\n",
    "            img_emb[base] = vec\n",
    "    return img_emb, cap_emb, cap2img\n",
    "\n",
    "def compute_top_k(similarities, ranked_keys, gt_keys, top_k):\n",
    "    metrics = {k: {\"accuracy\": 0, \"precision\": 0, \"recall\": 0} for k in top_k}\n",
    "    for k in top_k:\n",
    "        top_k_preds = ranked_keys[:k]\n",
    "        hits = len(set(top_k_preds) & set(gt_keys))\n",
    "        metrics[k][\"accuracy\"] = 1 if hits > 0 else 0\n",
    "        metrics[k][\"precision\"] = hits / k\n",
    "        metrics[k][\"recall\"] = hits / len(gt_keys)\n",
    "    return metrics\n",
    "\n",
    "def aggregate(acc, m):\n",
    "    for k in m:\n",
    "        for key in m[k]:\n",
    "            acc[k][key] += m[k][key]\n",
    "\n",
    "def report(title, metrics, total):\n",
    "    print(f\"\\n📌 {title} ({total} queries):\")\n",
    "    for k in TOP_K_VALUES:\n",
    "        a, p, r = metrics[k][\"accuracy\"]/total, metrics[k][\"precision\"]/total, metrics[k][\"recall\"]/total\n",
    "        print(f\"Top-{k}: Accuracy = {a:.4f} | Precision = {p:.4f} | Recall = {r:.4f}\")\n",
    "\n",
    "def _load_generic_captions(caption_file):\n",
    "    captions = defaultdict(list)\n",
    "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "        header_skipped = False\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if not header_skipped and (\"image\" in line.lower() and \"caption\" in line.lower()):\n",
    "                header_skipped = True\n",
    "                continue\n",
    "\n",
    "            # Detect separator and split\n",
    "            if '\\t' in line:\n",
    "                parts = line.split('\\t', 1)\n",
    "            else:\n",
    "                parts = line.split(',', 1)\n",
    "\n",
    "            if len(parts) != 2:\n",
    "                continue  # skip malformed lines\n",
    "\n",
    "            image_id, caption = parts\n",
    "            image_id = image_id.strip()\n",
    "            caption = caption.strip().strip('\"')\n",
    "            captions[image_id].append(caption)\n",
    "\n",
    "    return dict(captions)\n",
    "\n",
    "def load_flickr8k_captions(caption_file):\n",
    "    return _load_generic_captions(caption_file)\n",
    "\n",
    "def load_flickr30k_captions(caption_file):\n",
    "    return _load_generic_captions(caption_file)\n",
    "\n",
    "def evaluate(embedding_dir):\n",
    "    img_emb, cap_emb, cap2img = load_embeddings(embedding_dir)\n",
    "    cap_keys, img_keys = sorted(cap_emb), sorted(img_emb)\n",
    "    cap_matrix = torch.stack([cap_emb[k] for k in cap_keys]).numpy()\n",
    "    img_matrix = torch.stack([img_emb[k] for k in img_keys]).numpy()\n",
    "\n",
    "    # I2T\n",
    "    i2t = {k: {\"accuracy\": 0, \"precision\": 0, \"recall\": 0} for k in TOP_K_VALUES}\n",
    "    for img_id, img_vec in tqdm(img_emb.items(), desc=\"Image → Text\"):\n",
    "        sims = cosine_similarity(img_vec.unsqueeze(0).numpy(), cap_matrix)[0]\n",
    "        ranked = [cap_keys[i] for i in np.argsort(sims)[::-1]]\n",
    "        truth = cap2img[img_id]\n",
    "        metrics = compute_top_k(sims, ranked, truth, TOP_K_VALUES)\n",
    "        aggregate(i2t, metrics)\n",
    "    report(\"Image → Text\", i2t, len(img_emb))\n",
    "\n",
    "    # T2I\n",
    "    t2i = {k: {\"accuracy\": 0, \"precision\": 0, \"recall\": 0} for k in TOP_K_VALUES}\n",
    "    for cap_id, cap_vec in tqdm(cap_emb.items(), desc=\"Text → Image\"):\n",
    "        sims = cosine_similarity(cap_vec.unsqueeze(0).numpy(), img_matrix)[0]\n",
    "        ranked = [img_keys[i] for i in np.argsort(sims)[::-1]]\n",
    "        truth = [cap_id.split(\"_cap_\")[0]]\n",
    "        metrics = compute_top_k(sims, ranked, truth, TOP_K_VALUES)\n",
    "        aggregate(t2i, metrics)\n",
    "    report(\"Text → Image\", t2i, len(cap_emb))\n",
    "\n",
    "    # T2T\n",
    "    t2t = {k: {\"accuracy\": 0, \"precision\": 0, \"recall\": 0} for k in TOP_K_VALUES}\n",
    "    for i, (cap_id, cap_vec) in enumerate(tqdm(cap_emb.items(), desc=\"Text → Text\")):\n",
    "        sims = cosine_similarity(cap_vec.unsqueeze(0).numpy(), cap_matrix)[0]\n",
    "        sims[i] = -1e9\n",
    "        ranked = [cap_keys[j] for j in np.argsort(sims)[::-1]]\n",
    "        truth = [k for k in cap2img[cap_id.split(\"_cap_\")[0]] if k != cap_id]\n",
    "        metrics = compute_top_k(sims, ranked, truth, TOP_K_VALUES)\n",
    "        aggregate(t2t, metrics)\n",
    "    report(\"Text → Text\", t2t, len(cap_emb))\n",
    "\n",
    "# ---------- tiny helper ----------\n",
    "def show_image(path, title=\"\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=10)\n",
    "\n",
    "# ---------- 1. IMAGE ➜ TEXT ----------\n",
    "def image_to_text_retrieval(\n",
    "    image_id, image_embeddings, caption_embeddings, caption_keys,\n",
    "    top_k, image_dir, captions_dict\n",
    "):\n",
    "    print(\"\\n================ IMAGE → TEXT RETRIEVAL ================\")\n",
    "\n",
    "    query_vec   = image_embeddings[image_id].unsqueeze(0).cpu().numpy()\n",
    "    cap_matrix  = torch.stack([caption_embeddings[k] for k in caption_keys]).cpu().numpy()\n",
    "    sims        = cosine_similarity(query_vec, cap_matrix)[0]\n",
    "\n",
    "    ranked_idx  = np.argsort(sims)[::-1][:top_k]\n",
    "    ranked_caps = [caption_keys[i] for i in ranked_idx]\n",
    "\n",
    "    # show query image\n",
    "    show_image(os.path.join(image_dir, f\"{image_id}.jpg\"), \"Query Image\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nTop‑{top_k} captions:\")\n",
    "    print(f\"caption_list for {image_id}.jpg =\", captions_dict.get(f\"{image_id}.jpg\"))\n",
    "    for r, cap_key in enumerate(ranked_caps, 1):\n",
    "        base, num = cap_key.rsplit(\"_cap_\", 1)\n",
    "        caption = captions_dict.get(f\"{base}.jpg\", [\"[caption missing]\"]*5)[int(num)-1]\n",
    "        print(f\"{r}. {caption}\")\n",
    "\n",
    "# ---------- 2. TEXT ➜ IMAGE ----------\n",
    "def text_to_image_retrieval(\n",
    "    caption_key, image_embeddings, caption_embeddings, image_keys,\n",
    "    top_k, image_dir, captions_dict\n",
    "):\n",
    "    print(\"\\n================ TEXT → IMAGE RETRIEVAL ================\")\n",
    "\n",
    "    # Extract image file and caption index\n",
    "    base_img, cap_num = caption_key.rsplit(\"_cap_\", 1)\n",
    "    try:\n",
    "        caption_text = captions_dict[f\"{base_img}.jpg\"][int(cap_num) - 1]\n",
    "    except (IndexError, KeyError, ValueError):\n",
    "        caption_text = \"[Original caption not found]\"\n",
    "\n",
    "    print(f\"\\n📜 Query Caption: {caption_text}\")\n",
    "\n",
    "    query_vec   = caption_embeddings[caption_key].unsqueeze(0).cpu().numpy()\n",
    "    img_matrix  = torch.stack([image_embeddings[k] for k in image_keys]).cpu().numpy()\n",
    "    sims        = cosine_similarity(query_vec, img_matrix)[0]\n",
    "\n",
    "    ranked_idx  = np.argsort(sims)[::-1][:top_k]\n",
    "    ranked_imgs = [image_keys[i] for i in ranked_idx]\n",
    "\n",
    "    print(f\"\\nTop‑{top_k} images:\")\n",
    "    for r, img_id in enumerate(ranked_imgs, 1):\n",
    "        plt.figure()\n",
    "        show_image(os.path.join(image_dir, f\"{img_id}.jpg\"), f\"Rank {r}\")\n",
    "        plt.show()\n",
    "\n",
    "# ---------- 3. TEXT ➜ TEXT ----------\n",
    "def text_to_text_retrieval(\n",
    "    caption_key, caption_embeddings, caption_keys,\n",
    "    top_k, captions_dict\n",
    "):\n",
    "    print(\"\\n================ TEXT → TEXT RETRIEVAL ================\")\n",
    "\n",
    "    idx         = caption_keys.index(caption_key)\n",
    "    query_vec   = caption_embeddings[caption_key].unsqueeze(0).cpu().numpy()\n",
    "    cap_matrix  = torch.stack([caption_embeddings[k] for k in caption_keys]).cpu().numpy()\n",
    "    sims        = cosine_similarity(query_vec, cap_matrix)[0]\n",
    "    sims[idx]   = -1e9                                # drop self‑match\n",
    "\n",
    "    ranked_idx  = np.argsort(sims)[::-1][:top_k]\n",
    "    ranked_keys = [caption_keys[i] for i in ranked_idx]\n",
    "\n",
    "    base, num   = caption_key.rsplit(\"_cap_\", 1)\n",
    "    query_cap   = captions_dict.get(f\"{base}.jpg\", [\"[caption missing]\"]*5)[int(num)-1]\n",
    "    print(\"Query Caption:\", query_cap, \"\\n\")\n",
    "\n",
    "    print(f\"Top‑{top_k} similar captions:\")\n",
    "    for r, key in enumerate(ranked_keys, 1):\n",
    "        b, n   = key.rsplit(\"_cap_\", 1)\n",
    "        cap    = captions_dict.get(f\"{b}.jpg\", [\"[caption missing]\"]*5)[int(n)-1]\n",
    "        print(f\"{r}. {cap}\")\n",
    "\n",
    "# ---------- 4. IMAGE ➜ IMAGE ----------\n",
    "def image_to_image_retrieval(\n",
    "    image_id, image_embeddings, image_keys,\n",
    "    top_k, image_dir\n",
    "):\n",
    "    print(\"\\n================ IMAGE → IMAGE RETRIEVAL ================\")\n",
    "\n",
    "    query_vec   = image_embeddings[image_id].unsqueeze(0).cpu().numpy()\n",
    "    img_matrix  = torch.stack([image_embeddings[k] for k in image_keys]).cpu().numpy()\n",
    "    sims        = cosine_similarity(query_vec, img_matrix)[0]\n",
    "    sims[image_keys.index(image_id)] = -1e9           # drop self‑match\n",
    "\n",
    "    ranked_idx  = np.argsort(sims)[::-1][:top_k]\n",
    "    ranked_imgs = [image_keys[i] for i in ranked_idx]\n",
    "\n",
    "    # show query\n",
    "    show_image(os.path.join(image_dir, f\"{image_id}.jpg\"), \"Query Image\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nTop‑{top_k} similar images:\")\n",
    "    for r, img_id in enumerate(ranked_imgs, 1):\n",
    "        plt.figure()\n",
    "        show_image(os.path.join(image_dir, f\"{img_id}.jpg\"), f\"Rank {r}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6cJqEM5mC7LT",
    "outputId": "cc35bbe2-97c0-4ae1-949a-a6a13986aad8"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# === CONTROL FLAGS ===\n",
    "evaluate_flag = False  # Set to True to evaluation accuracy metrics\n",
    "\n",
    "# === DATASET CONFIGURATION ===\n",
    "datasets = {\n",
    "    \"flickr8k\": {\n",
    "        \"image_dir\": \"/content/sample_data/flickr8k/Images\",\n",
    "        \"caption_file\": \"/content/sample_data/flickr8k/captions.txt\",\n",
    "        \"embedding_dir\": \"/content/sample_data/flickr8k/embeddings\",\n",
    "        \"zip_path\": \"/content/drive/MyDrive/Flickr8k_embeddings_openai_clip.zip\"\n",
    "    },\n",
    "    \"flickr30k\": {\n",
    "        \"image_dir\": \"/content/sample_data/flickr30k/Images\",\n",
    "        \"caption_file\": \"/content/sample_data/flickr30k/captions.txt\",\n",
    "        \"embedding_dir\": \"/content/sample_data/flickr30k/embeddings\",\n",
    "        \"zip_path\": \"/content/drive/MyDrive/Flickr30k_embeddings_openai_clip.zip\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# === CONDITIONAL EMBEDDING EXTRACTION ===\n",
    "def maybe_extract_embeddings(zip_path, extract_to):\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"📦 Found precomputed embeddings: {os.path.basename(zip_path)}. Extracting...\")\n",
    "        os.makedirs(extract_to, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for name, cfg in datasets.items():\n",
    "    print(f\"\\n🚀 Running pipeline for {name.upper()}\")\n",
    "\n",
    "    # Check for precomputed embeddings\n",
    "    if not maybe_extract_embeddings(cfg[\"zip_path\"], cfg[\"embedding_dir\"]):\n",
    "        print(f\"🔄 Embeddings not found for {name}. Generating from scratch...\")\n",
    "\n",
    "        # === Generate embeddings ===\n",
    "        generate_embeddings(cfg[\"image_dir\"], cfg[\"caption_file\"], cfg[\"embedding_dir\"])\n",
    "\n",
    "        # === Zip generated embeddings and download ===\n",
    "        zip_output = f\"/content/{name}_embeddings.zip\"\n",
    "        shutil.make_archive(base_name=zip_output.replace(\".zip\", \"\"), format='zip', root_dir=cfg[\"embedding_dir\"])\n",
    "        print(f\"📦 Zipped embeddings to {zip_output}\")\n",
    "\n",
    "        try:\n",
    "            files.download(zip_output)\n",
    "            print(\"⬇️ Triggered download of generated embeddings.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Download failed: {e}\")\n",
    "    else:\n",
    "        print(f\"✅ Using precomputed embeddings for {name}.\")\n",
    "\n",
    "    # === Optional Evaluation ===\n",
    "    if evaluate_flag:\n",
    "        print(f\"📊 Evaluating {name} embeddings...\")\n",
    "        evaluate(cfg[\"embedding_dir\"])\n",
    "    else:\n",
    "        print(f\"⏩ Skipping evaluation for {name} (flag is off)\")\n",
    "\n",
    "    # === Load embeddings & captions ===\n",
    "    image_embeddings, caption_embeddings, caption_to_image = load_embeddings(cfg[\"embedding_dir\"])\n",
    "    caption_keys = sorted(caption_embeddings.keys())\n",
    "    image_keys = sorted(image_embeddings.keys())\n",
    "\n",
    "    if name == \"flickr8k\":\n",
    "        captions_dict = load_flickr8k_captions(cfg[\"caption_file\"])\n",
    "    else:\n",
    "        captions_dict = load_flickr30k_captions(cfg[\"caption_file\"])\n",
    "\n",
    "    # === Select sample and run retrieval ===\n",
    "    sample_image_id = image_keys[0]\n",
    "    sample_caption_key = [k for k in caption_keys if k.startswith(sample_image_id)][0]\n",
    "\n",
    "    print(f\"\\n🎯 Retrieval demo for {name.upper()} — Sample: {sample_image_id}\")\n",
    "\n",
    "    image_to_text_retrieval(sample_image_id, image_embeddings, caption_embeddings,\n",
    "                              caption_keys, 3, cfg[\"image_dir\"], captions_dict)\n",
    "\n",
    "    text_to_image_retrieval(sample_caption_key, image_embeddings, caption_embeddings,\n",
    "                              image_keys, 3, cfg[\"image_dir\"], captions_dict)\n",
    "\n",
    "    text_to_text_retrieval(sample_caption_key, caption_embeddings, caption_keys,\n",
    "                           3, captions_dict)\n",
    "\n",
    "    image_to_image_retrieval(sample_image_id, image_embeddings, image_keys,\n",
    "                            3, cfg[\"image_dir\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
