{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restructured Multimodal Retrieval Notebook\n",
    "### *Flexible, Efficient, and Portable for Flickr8k/30k Datasets*\n",
    "\n",
    "This notebook has been refactored into a professional research pipeline. Key features and improvements include:\n",
    "- **Experiment-Driven Configuration**: A master `experiment_configs` dictionary allows for defining specific, fine-tuned hyperparameters for every model architecture and dataset combination.\n",
    "- **Flexible Model Factory**: The architecture is fully modular, allowing for easy experimentation with different encoders (e.g., ResNet, ViT, RoBERTa, DistilBERT) by simply changing a model name in the configuration.\n",
    "- **Automated & Portable Workflow**: The script auto-detects the execution environment (Colab, RunPod, local) and generates a standardized directory structure for data, models, and embeddings.\n",
    "- **Robust Data Pipeline**: Correctly downloads and processes both Flickr8k and the multi-part Flickr30k dataset, including caption standardization, directly in the target directory without unnecessary file moving.\n",
    "- **Advanced Training Techniques**: Incorporates a `Trainer` class, gradient accumulation, early stopping, caption sampling for regularization, and a cosine annealing learning rate scheduler.\n",
    "- **Efficient Checkpointing & Controls**: Skips re-downloading, re-training, and re-generating embeddings if artifacts already exist. Includes flags like `force_retrain`, `run_embedding_generation`, and `run_evaluation` for fine-grained control.\n",
    "- **Comprehensive Evaluation**: Performs bidirectional (Text-to-Image, Image-to-Text, and Text-to-Text) retrieval evaluation using Top-K (1-5) accuracy, precision, and recall.\n",
    "- **Qualitative & Comparative Analysis**: Generates qualitative examples of retrieval results and provides side-by-side plots and summary tables for easy comparison across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Imports and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm pandas tqdm albumentations opencv-python scikit-learn transformers torch torchvision torchaudio\n",
    "!pip install kaggle matplotlib\n",
    "!pip install -U ipywidgets\n",
    "\n",
    "# Specific to RunPod to avoid unknown command Unzip\n",
    "!apt update && apt install zip -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import timm\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Experiment Configurations**\n",
    "This is the master control panel. A `BaseCFG` holds common parameters. The `experiment_configs` dictionary defines specific models and fine-tuned hyperparameters for each model-dataset combination.\n",
    "\n",
    "#### **Workflow Control:**\n",
    "- **Train Only**: `force_retrain = True`, `run_embedding_generation = False`, `run_evaluation = False`\n",
    "- **Train & Generate Embeddings**: `force_retrain = True`, `run_embedding_generation = True`, `run_evaluation = False`\n",
    "- **Evaluate Existing Embeddings Only**: `force_retrain = False`, `run_embedding_generation = True` (will skip if files exist), `run_evaluation = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCFG:\n",
    "    # --- Core Hyperparameters & Controls ---\n",
    "    debug = False\n",
    "    epochs = 30\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    gradient_accumulation_steps = 2\n",
    "    early_stopping_patience = 7\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    force_retrain = False\n",
    "    run_embedding_generation = True\n",
    "    run_evaluation = True\n",
    "    show_comparative_plots = True\n",
    "\n",
    "    # --- LR and Scheduler ---\n",
    "    warmup_epochs = 5\n",
    "    lr_min = 1e-6\n",
    "\n",
    "    # --- Model Configuration ---\n",
    "    size = 224\n",
    "    max_length = 200\n",
    "    pretrained = True\n",
    "    trainable = True\n",
    "\n",
    "experiment_configs = {\n",
    "    \"resnet50_distilbert\": {\n",
    "        \"models\": {\n",
    "            \"image_model_name\": \"resnet50\",\n",
    "            \"text_encoder_model\": \"distilbert-base-uncased\",\n",
    "            \"text_tokenizer\": \"distilbert-base-uncased\"\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"flickr8k\": { # Reverted to stable parameters\n",
    "                \"head_lr\": 1e-3, \"image_encoder_lr\": 1e-4, \"text_encoder_lr\": 2e-5, \n",
    "                \"weight_decay\": 1e-3, \"projection_dim\": 256, \"dropout\": 0.1, \"temperature\": 0.07\n",
    "            },\n",
    "            \"flickr30k\": {\n",
    "                \"head_lr\": 1e-3, \"image_encoder_lr\": 1e-4, \"text_encoder_lr\": 2e-5, \n",
    "                \"weight_decay\": 1e-3, \"projection_dim\": 256, \"dropout\": 0.1, \"temperature\": 0.07\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"vit_roberta\": {\n",
    "        \"models\": {\n",
    "            \"image_model_name\": \"vit_base_patch16_224\",\n",
    "            \"text_encoder_model\": \"roberta-base\",\n",
    "            \"text_tokenizer\": \"roberta-base\"\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"flickr8k\": {\n",
    "                \"head_lr\": 5e-4, \"image_encoder_lr\": 1e-5, \"text_encoder_lr\": 5e-6,\n",
    "                \"weight_decay\": 2e-3, \"projection_dim\": 512, \"dropout\": 0.2, \"temperature\": 0.1\n",
    "            },\n",
    "            \"flickr30k\": {\n",
    "                \"head_lr\": 1e-3, \"image_encoder_lr\": 5e-5, \"text_encoder_lr\": 2e-5, \n",
    "                \"weight_decay\": 1e-3, \"projection_dim\": 512, \"dropout\": 0.2, \"temperature\": 0.1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Environment, Path, and Data Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_environment():\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"Environment: Google Colab detected.\")\n",
    "        return \"colab\", \"/content\"\n",
    "    except ImportError: pass\n",
    "    if os.path.exists(\"/workspace\") or \"RUNPOD_POD_ID\" in os.environ:\n",
    "        print(\"Environment: RunPod detected.\")\n",
    "        return \"runpod\", \"/workspace\"\n",
    "    print(\"Environment: Local machine detected.\")\n",
    "    return \"local\", os.getcwd()\n",
    "\n",
    "def generate_paths(base_path, dataset_name, cfg):\n",
    "    dataset_dir = os.path.join(base_path, \"data\", dataset_name)\n",
    "    models_dir = os.path.join(base_path, \"models\")\n",
    "    embeddings_dir = os.path.join(base_path, \"embeddings\")\n",
    "    paths = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"dataset_dir\": dataset_dir,\n",
    "        \"image_dir\": os.path.join(dataset_dir, \"Images\"),\n",
    "        \"captions_file\": os.path.join(dataset_dir, f\"{dataset_name}_captions.csv\"),\n",
    "        \"model_save_path\": os.path.join(models_dir, f\"{dataset_name}_{cfg.image_model_name.replace('/', '-')}_{cfg.text_encoder_model.replace('/', '-')}.pt\"),\n",
    "        \"embedding_save_path\": os.path.join(embeddings_dir, dataset_name, f\"{cfg.image_model_name.replace('/', '-')}_{cfg.text_encoder_model.replace('/', '-')}\"),\n",
    "    }\n",
    "    for path in [paths[\"image_dir\"], os.path.dirname(paths[\"model_save_path\"]), paths[\"embedding_save_path\"]]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    return paths\n",
    "\n",
    "def run_shell_command(command):\n",
    "    try:\n",
    "        print(f\"Running command: {' '.join(command)}\")\n",
    "        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error executing command: {' '.join(command)}\")\n",
    "        print(e.stderr)\n",
    "        raise\n",
    "\n",
    "def download_with_progress(url, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=f\"Downloading {os.path.basename(filename)}\", total=total, unit='B', unit_scale=True, unit_divisor=1024\n",
    "    ) as bar:\n",
    "        for chunk in resp.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "def download_flickr8k(target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(\"📥 Downloading flickr8k...\")\n",
    "    zip_path = os.path.join(target_dir, \"flickr8k.zip\")\n",
    "    url = \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip\"\n",
    "    download_with_progress(url, zip_path)\n",
    "    run_shell_command([f\"unzip -q -o {zip_path} -d {target_dir}\"])\n",
    "    os.remove(zip_path)\n",
    "\n",
    "def download_flickr30k(target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(\"📥 Downloading flickr30k...\")\n",
    "    zip_path = os.path.join(target_dir, \"flickr30k.zip\")\n",
    "    parts = [f\"flickr30k_part0{i}\" for i in range(3)]\n",
    "    urls = [f\"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/{p}\" for p in parts]\n",
    "    part_paths = [os.path.join(target_dir, p) for p in parts]\n",
    "    for url, part_path in zip(urls, part_paths):\n",
    "        download_with_progress(url, part_path)\n",
    "    \n",
    "    run_shell_command([f\"cat {' '.join(part_paths)} > {zip_path}\"])\n",
    "    for part in part_paths:\n",
    "        os.remove(part)\n",
    "    run_shell_command([f\"unzip -q -o {zip_path} -d {target_dir}\"])\n",
    "    os.remove(zip_path)\n",
    "\n",
    "def clean_caption(text):\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)          # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)                 # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "def process_captions(raw_captions_path, final_captions_path):\n",
    "    print(f\"Processing captions from {raw_captions_path}...\")\n",
    "    if not os.path.exists(raw_captions_path):\n",
    "        print(f\"❌ Missing raw captions file: {raw_captions_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(raw_captions_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.rename(columns={\"image_name\": \"image\", \"comment\": \"caption\"}, inplace=True)\n",
    "    df.dropna(subset=[\"caption\"], inplace=True)\n",
    "    df[\"caption\"] = df[\"caption\"].astype(str).str.strip().apply(clean_caption)\n",
    "    df[\"num_tokens\"] = df[\"caption\"].apply(lambda x: len(x.split()))\n",
    "    df = df[(df[\"num_tokens\"] >= 3) & (df[\"num_tokens\"] <= 50)].reset_index(drop=True)\n",
    "\n",
    "    df[\"caption_number\"] = df.groupby(\"image\").cumcount()\n",
    "    df[\"id\"] = df[\"image\"].factorize()[0]\n",
    "    df = df[[\"image\", \"caption_number\", \"caption\", \"id\"]]\n",
    "\n",
    "    df.to_csv(final_captions_path, index=False)\n",
    "    print(f\"\\n✅ Preprocessing DONE\")\n",
    "    print(f\"📝 Total captions: {len(df)}\")\n",
    "    print(f\"🔤 Avg length: {df['caption'].apply(lambda x: len(x.split())).mean():.2f} tokens\")\n",
    "    print(f\"📄 Saved: {final_captions_path}\")\n",
    "\n",
    "def prepare_dataset(config):\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    dataset_dir = config[\"dataset_dir\"]\n",
    "    image_dir = config[\"image_dir\"]\n",
    "    captions_file = config[\"captions_file\"]\n",
    "\n",
    "    if os.path.exists(captions_file) and os.path.exists(image_dir) and len(glob.glob(os.path.join(image_dir, '*.jpg'))) > 10:\n",
    "        print(f\"Dataset '{dataset_name}' found and processed. Skipping preparation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Dataset '{dataset_name}' not found. Starting download and preparation...\")\n",
    "    \n",
    "    if dataset_name == 'flickr8k':\n",
    "        download_flickr8k(dataset_dir)\n",
    "    elif dataset_name == 'flickr30k':\n",
    "        download_flickr30k(dataset_dir)\n",
    "    else:\n",
    "        print(f\"Unknown dataset: {dataset_name}\")\n",
    "        return\n",
    "\n",
    "    raw_captions_path = os.path.join(dataset_dir, 'captions.txt')\n",
    "    process_captions(raw_captions_path, captions_file)\n",
    "    \n",
    "    if os.path.exists(raw_captions_path):\n",
    "        os.remove(raw_captions_path)\n",
    "    \n",
    "    print(f\"Dataset '{dataset_name}' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Data Handling Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, image_paths, captions, tokenizer, transforms, cfg):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = list(captions)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transforms = transforms\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded_caption = self.tokenizer(\n",
    "            self.captions[idx], padding='max_length', truncation=True, \n",
    "            max_length=self.cfg.max_length, return_tensors='pt'\n",
    "        )\n",
    "        try:\n",
    "            image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "        image = np.array(image)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        \n",
    "        item = {\n",
    "            'image': image,\n",
    "            'input_ids': encoded_caption['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded_caption['attention_mask'].squeeze(0),\n",
    "            'image_path': self.image_paths[idx],\n",
    "            'caption': self.captions[idx]\n",
    "        }\n",
    "        return item\n",
    "\n",
    "def get_transforms(cfg, mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose([\n",
    "            A.RandomResizedCrop(size=(cfg.size, cfg.size), scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=cfg.size, width=cfg.size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "def make_train_valid_dfs(config):\n",
    "    try:\n",
    "        df = pd.read_csv(config['captions_file'])\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading captions file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    image_files_in_dir = set(os.listdir(config['image_dir']))\n",
    "    df = df[df['image'].isin(image_files_in_dir)].reset_index(drop=True)\n",
    "\n",
    "    unique_images = df['image'].unique()\n",
    "    train_imgs, valid_imgs = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_df = df[df['image'].isin(train_imgs)].reset_index(drop=True)\n",
    "    valid_df = df[df['image'].isin(valid_imgs)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, valid_df\n",
    "\n",
    "def build_loaders(df, tokenizer, mode, config, cfg):\n",
    "    transforms = get_transforms(cfg, mode=mode)\n",
    "    image_paths = [os.path.join(config['image_dir'], fname) for fname in df['image'].values]\n",
    "    captions = df['caption'].values\n",
    "    \n",
    "    dataset = CLIPDataset(image_paths, captions, tokenizer, transforms, cfg)\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        batch = [item for item in batch if item is not None]\n",
    "        if not batch:\n",
    "            return None\n",
    "        return torch.utils.data.dataloader.default_collate(batch)\n",
    "        \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        num_workers=cfg.num_workers, \n",
    "        shuffle=True if mode == 'train' else False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Model, Loss, and Trainer Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_encoder(cfg): return timm.create_model(cfg.image_model_name, pretrained=cfg.pretrained, num_classes=0, global_pool='avg')\n",
    "def create_text_encoder(cfg): return AutoModel.from_pretrained(cfg.text_encoder_model)\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, cfg):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, cfg.projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(cfg.projection_dim, cfg.projection_dim)\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "        self.layer_norm = nn.LayerNorm(cfg.projection_dim)\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x) + projected\n",
    "        return self.layer_norm(x)\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, cfg):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        image_embedding_dim = image_encoder.num_features\n",
    "        text_embedding_dim = text_encoder.config.hidden_size\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding_dim, cfg=cfg)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding_dim, cfg=cfg)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        image_features = self.image_encoder(batch['image'])\n",
    "        text_features = self.text_encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).last_hidden_state[:, 0, :]\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "        return image_embeddings, text_embeddings\n",
    "\n",
    "class CLIPLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        targets = torch.arange(len(image_embeddings)).to(image_embeddings.device)\n",
    "        texts_loss = F.cross_entropy(logits, targets)\n",
    "        images_loss = F.cross_entropy(logits.T, targets)\n",
    "        return (images_loss + texts_loss) / 2.0\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"): self.name, self.avg, self.sum, self.count = name, 0, 0, 0\n",
    "    def update(self, val, n=1): self.sum += val * n; self.count += n; self.avg = self.sum / self.count\n",
    "    def __repr__(self): return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler, loss_fn, cfg):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.loss_fn = loss_fn\n",
    "        self.cfg = cfg\n",
    "        self.device = cfg.device\n",
    "\n",
    "    def _train_one_epoch(self, train_loader):\n",
    "        loss_meter = AvgMeter()\n",
    "        tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "        self.model.train()\n",
    "        for i, batch in enumerate(tqdm_object):\n",
    "            if batch is None: continue\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            image_embeddings, text_embeddings = self.model(batch)\n",
    "            loss = self.loss_fn(image_embeddings, text_embeddings)\n",
    "            loss = loss / self.cfg.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "            if (i + 1) % self.cfg.gradient_accumulation_steps == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                if self.scheduler: self.scheduler.step()\n",
    "\n",
    "            loss_meter.update(loss.item() * self.cfg.gradient_accumulation_steps, batch['image'].size(0))\n",
    "            tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=self.optimizer.param_groups[0]['lr'])\n",
    "        return loss_meter\n",
    "\n",
    "    def _valid_one_epoch(self, valid_loader):\n",
    "        loss_meter = AvgMeter()\n",
    "        tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm_object:\n",
    "                if batch is None: continue\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "                image_embeddings, text_embeddings = self.model(batch)\n",
    "                loss = self.loss_fn(image_embeddings, text_embeddings)\n",
    "                loss_meter.update(loss.item(), batch['image'].size(0))\n",
    "                tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "        return loss_meter\n",
    "\n",
    "    def fit(self, train_df, valid_loader, tokenizer, config):\n",
    "        best_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        history = defaultdict(list)\n",
    "        total_training_start_time = time.time()\n",
    "\n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            print(f\"\\nEpoch: {epoch + 1}/{self.cfg.epochs}\")\n",
    "            \n",
    "            epoch_train_df = train_df.groupby('id').sample(n=1).reset_index(drop=True)\n",
    "            epoch_train_loader = build_loaders(epoch_train_df, tokenizer, mode=\"train\", config=config, cfg=self.cfg)\n",
    "\n",
    "            train_loss = self._train_one_epoch(epoch_train_loader)\n",
    "            valid_loss = self._valid_one_epoch(valid_loader)\n",
    "\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            history['train_loss'].append(train_loss.avg)\n",
    "            history['valid_loss'].append(valid_loss.avg)\n",
    "            history['epoch_times'].append(epoch_duration)\n",
    "\n",
    "            print(f\"Epoch {epoch+1} | Train Loss: {train_loss.avg:.4f} | Valid Loss: {valid_loss.avg:.4f} | Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "            if valid_loss.avg < best_loss:\n",
    "                best_loss = valid_loss.avg\n",
    "                epochs_without_improvement = 0\n",
    "                torch.save({'epoch': epoch + 1, 'model_state_dict': self.model.state_dict()}, config['model_save_path'])\n",
    "                print(f\"Saved Best Model! Validation Loss: {best_loss:.4f}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                print(f\"No improvement in validation loss for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "            if epochs_without_improvement >= self.cfg.early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {self.cfg.early_stopping_patience} epochs without improvement.\")\n",
    "                break\n",
    "        \n",
    "        total_training_end_time = time.time()\n",
    "        total_training_duration = total_training_end_time - total_training_start_time\n",
    "        \n",
    "        plot_loss_curves(history, config)\n",
    "        print_performance_summary(history, total_training_duration, epoch_train_loader, self.cfg, config)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Embedding Generation and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_embeddings(model, dataloader, config, cfg, set_name):\n",
    "    start_time = time.time()\n",
    "    print(f\"Generating embeddings for {config['dataset_name']} {set_name} set...\")\n",
    "    model.eval()\n",
    "    \n",
    "    all_image_embeddings, all_text_embeddings = [], []\n",
    "    all_image_paths, all_captions = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            if batch is None: continue\n",
    "            image_embeddings, text_embeddings = model({\n",
    "                'image': batch['image'].to(cfg.device),\n",
    "                'input_ids': batch['input_ids'].to(cfg.device),\n",
    "                'attention_mask': batch['attention_mask'].to(cfg.device)\n",
    "            })\n",
    "            all_image_embeddings.append(image_embeddings.cpu())\n",
    "            all_text_embeddings.append(text_embeddings.cpu())\n",
    "            all_image_paths.extend(batch['image_path'])\n",
    "            all_captions.extend(batch['caption'])\n",
    "\n",
    "    image_embeddings_full = torch.cat(all_image_embeddings)\n",
    "    text_embeddings_full = torch.cat(all_text_embeddings)\n",
    "    \n",
    "    unique_image_paths = sorted(list(set(all_image_paths)))\n",
    "    path_to_idx = {path: i for i, path in enumerate(unique_image_paths)}\n",
    "    \n",
    "    unique_image_embeddings = torch.zeros(len(unique_image_paths), cfg.projection_dim)\n",
    "    path_to_embedding_map = defaultdict(list)\n",
    "    for i in range(len(all_image_paths)):\n",
    "        path_to_embedding_map[all_image_paths[i]].append(image_embeddings_full[i])\n",
    "    \n",
    "    for i, path in enumerate(unique_image_paths):\n",
    "        unique_image_embeddings[i] = torch.stack(path_to_embedding_map[path]).mean(dim=0)\n",
    "        \n",
    "    ground_truth = torch.tensor([path_to_idx[path] for path in all_image_paths])\n",
    "    \n",
    "    embedding_dir = config['embedding_save_path']\n",
    "    torch.save(unique_image_embeddings, os.path.join(embedding_dir, f'{set_name}_image_embeddings.pt'))\n",
    "    torch.save(text_embeddings_full, os.path.join(embedding_dir, f'{set_name}_text_embeddings.pt'))\n",
    "    torch.save(ground_truth, os.path.join(embedding_dir, f'{set_name}_ground_truth.pt'))\n",
    "    torch.save(all_captions, os.path.join(embedding_dir, f'{set_name}_captions.pt'))\n",
    "    torch.save(unique_image_paths, os.path.join(embedding_dir, f'{set_name}_unique_image_paths.pt'))\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"{set_name.capitalize()} embeddings saved to '{embedding_dir}'. Time taken: {duration:.2f} seconds.\")\n",
    "\n",
    "def compute_top_k_retrieval_metrics(ranked_indices, ground_truth_indices, k_values):\n",
    "    metrics = {k: {\"accuracy\": 0, \"precision\": 0, \"recall\": 0} for k in k_values}\n",
    "    ground_truth_set = set(ground_truth_indices)\n",
    "    if not ground_truth_set: return metrics\n",
    "    \n",
    "    for k in k_values:\n",
    "        top_k_preds = set(ranked_indices[:k])\n",
    "        hits = len(top_k_preds.intersection(ground_truth_set))\n",
    "        metrics[k][\"accuracy\"] = 1 if hits > 0 else 0\n",
    "        metrics[k][\"precision\"] = hits / k if k > 0 else 0\n",
    "        metrics[k][\"recall\"] = hits / len(ground_truth_set) if len(ground_truth_set) > 0 else 0\n",
    "    return metrics\n",
    "\n",
    "def report_retrieval_metrics(title, accumulated_metrics, total_queries, k_values):\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    if total_queries == 0:\n",
    "        print(\"No valid queries processed.\")\n",
    "        return {}\n",
    "        \n",
    "    header = f\"| {'Top-K':<5} | {'Accuracy':<10} | {'Precision':<10} | {'Recall':<10} |\"\n",
    "    print(header)\n",
    "    print(\"|\" + \"-\" * (len(header) - 2) + \"|\")\n",
    "    final_metrics = {}\n",
    "    for k in k_values:\n",
    "        acc = accumulated_metrics[k]['accuracy'] / total_queries\n",
    "        prec = accumulated_metrics[k]['precision'] / total_queries\n",
    "        rec = accumulated_metrics[k]['recall'] / total_queries\n",
    "        final_metrics[k] = {'accuracy': acc, 'precision': prec, 'recall': rec}\n",
    "        row = f\"| {k:<5} | {acc:<10.4f} | {prec:<10.4f} | {rec:<10.4f} |\"\n",
    "        print(row)\n",
    "    print(\"-\" * len(header))\n",
    "    return final_metrics\n",
    "\n",
    "def evaluate_retrieval(image_embeddings, text_embeddings, ground_truth_map, config, cfg):\n",
    "    TOP_K_VALUES = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    image_embeddings_gpu = F.normalize(image_embeddings.float().to(cfg.device), p=2, dim=-1)\n",
    "    text_embeddings_gpu = F.normalize(text_embeddings.float().to(cfg.device), p=2, dim=-1)\n",
    "    ground_truth_map_np = ground_truth_map.numpy()\n",
    "\n",
    "    # --- Text-to-Image Retrieval ---\n",
    "    t2i_metrics = {k: defaultdict(float) for k in TOP_K_VALUES}\n",
    "    num_text_queries = len(text_embeddings_gpu)\n",
    "    for i in tqdm(range(num_text_queries), desc=\"Text-to-Image Retrieval\"):\n",
    "        query_embedding = text_embeddings_gpu[i].unsqueeze(0)\n",
    "        sim = query_embedding @ image_embeddings_gpu.T\n",
    "        ranked_indices = torch.argsort(sim, descending=True).squeeze().cpu().numpy().tolist()\n",
    "        if not isinstance(ranked_indices, list): ranked_indices = [ranked_indices]\n",
    "        query_metrics = compute_top_k_retrieval_metrics(ranked_indices, [ground_truth_map_np[i]], TOP_K_VALUES)\n",
    "        for k in TOP_K_VALUES:\n",
    "            for metric_name in query_metrics[k]: t2i_metrics[k][metric_name] += query_metrics[k][metric_name]\n",
    "    t2i_final_metrics = report_retrieval_metrics(f\"Text-to-Image Retrieval ({config['dataset_name']}) - {cfg.image_model_name} + {cfg.text_encoder_model}\", t2i_metrics, num_text_queries, TOP_K_VALUES)\n",
    "\n",
    "    # --- Image-to-Text Retrieval ---\n",
    "    image_to_captions = defaultdict(list)\n",
    "    for caption_idx, image_idx in enumerate(ground_truth_map_np):\n",
    "        image_to_captions[image_idx].append(caption_idx)\n",
    "    i2t_metrics = {k: defaultdict(float) for k in TOP_K_VALUES}\n",
    "    num_image_queries = len(image_embeddings_gpu)\n",
    "    for i in tqdm(range(num_image_queries), desc=\"Image-to-Text Retrieval\"):\n",
    "        query_embedding = image_embeddings_gpu[i].unsqueeze(0)\n",
    "        sim = query_embedding @ text_embeddings_gpu.T\n",
    "        ranked_indices = torch.argsort(sim, descending=True).squeeze().cpu().numpy().tolist()\n",
    "        if not isinstance(ranked_indices, list): ranked_indices = [ranked_indices]\n",
    "        ground_truth_caption_indices = image_to_captions[i]\n",
    "        query_metrics = compute_top_k_retrieval_metrics(ranked_indices, ground_truth_caption_indices, TOP_K_VALUES)\n",
    "        for k in TOP_K_VALUES:\n",
    "            for metric_name in query_metrics[k]: i2t_metrics[k][metric_name] += query_metrics[k][metric_name]\n",
    "    i2t_final_metrics = report_retrieval_metrics(f\"Image-to-Text Retrieval ({config['dataset_name']}) - {cfg.image_model_name} + {cfg.text_encoder_model}\", i2t_metrics, num_image_queries, TOP_K_VALUES)\n",
    "\n",
    "    # --- Text-to-Text Retrieval ---\n",
    "    t2t_metrics = {k: defaultdict(float) for k in TOP_K_VALUES}\n",
    "    for i in tqdm(range(num_text_queries), desc=\"Text-to-Text Retrieval\"):\n",
    "        query_embedding = text_embeddings_gpu[i].unsqueeze(0)\n",
    "        sim = query_embedding @ text_embeddings_gpu.T\n",
    "        sim[0, i] = -torch.inf\n",
    "        ranked_indices = torch.argsort(sim, descending=True).squeeze().cpu().numpy().tolist()\n",
    "        if not isinstance(ranked_indices, list): ranked_indices = [ranked_indices]\n",
    "        ground_truth_image_idx = ground_truth_map_np[i]\n",
    "        ground_truth_caption_indices = image_to_captions[ground_truth_image_idx]\n",
    "        query_metrics = compute_top_k_retrieval_metrics(ranked_indices, ground_truth_caption_indices, TOP_K_VALUES)\n",
    "        for k in TOP_K_VALUES:\n",
    "            for metric_name in query_metrics[k]: t2t_metrics[k][metric_name] += query_metrics[k][metric_name]\n",
    "    t2t_final_metrics = report_retrieval_metrics(f\"Text-to-Text Retrieval ({config['dataset_name']}) - {cfg.image_model_name} + {cfg.text_encoder_model}\", t2t_metrics, num_text_queries, TOP_K_VALUES)\n",
    "\n",
    "    return {\"t2i\": t2i_final_metrics, \"i2t\": i2t_final_metrics, \"t2t\": t2t_final_metrics}\n",
    "\n",
    "def plot_loss_curves(training_history, config):\n",
    "    if not training_history: return\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(range(1, len(training_history['train_loss']) + 1), training_history['train_loss'], label='Train Loss')\n",
    "    plt.plot(range(1, len(training_history['valid_loss']) + 1), training_history['valid_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss - {config[\"dataset_name\"]}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(1, len(training_history['train_loss']) + 1))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def print_performance_summary(training_history, total_training_duration, train_loader, cfg, config):\n",
    "    if not training_history: return\n",
    "    total_epochs_trained = len(training_history['train_loss'])\n",
    "    avg_epoch_time = sum(training_history['epoch_times']) / len(training_history['epoch_times']) if training_history['epoch_times'] else 0\n",
    "    iterations_per_epoch = len(train_loader)\n",
    "    avg_iteration_time = avg_epoch_time / iterations_per_epoch if iterations_per_epoch > 0 else 0\n",
    "\n",
    "    print(f\"\\n--- Training Performance Summary for {config['dataset_name']} ---\")\n",
    "    print(f\"  GPU Used:                  {torch.cuda.get_device_name(0) if cfg.device.type == 'cuda' else 'CPU'}\")\n",
    "    print(f\"  Total Epochs Trained:      {total_epochs_trained}\")\n",
    "    print(f\"  Batch Size:                {cfg.batch_size}\")\n",
    "    print(f\"  Head Learning Rate:        {cfg.head_lr}\")\n",
    "    print(f\"  Image Encoder LR:          {cfg.image_encoder_lr}\")\n",
    "    print(f\"  Text Encoder LR:           {cfg.text_encoder_lr}\")\n",
    "    print(f\"  Optimizer:                 AdamW\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(f\"  Total Training Time:       {total_training_duration:.2f} seconds ({total_training_duration/60:.2f} minutes)\")\n",
    "    print(f\"  Average Time per Epoch:    {avg_epoch_time:.2f} seconds\")\n",
    "    print(f\"  Average Time per Iteration:{avg_iteration_time:.4f} seconds\")\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7: The Main Pipeline Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(config, cfg):\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"STARTING PIPELINE FOR: {config['dataset_name'].upper()}\")\n",
    "    print(f\"With model: {cfg.image_model_name} + {cfg.text_encoder_model}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # --- Setup: Data Loading ---\n",
    "    print(\"Setting up datasets and dataloaders...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.text_tokenizer)\n",
    "    train_df, valid_df = make_train_valid_dfs(config)\n",
    "    if train_df is None or valid_df is None or train_df.empty or valid_df.empty:\n",
    "        print(\"Could not create dataframes or they are empty. Aborting pipeline.\")\n",
    "        return None, None\n",
    "    \n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\", config=config, cfg=cfg)\n",
    "\n",
    "    # --- Model Creation and Loading ---\n",
    "    image_encoder = create_image_encoder(cfg)\n",
    "    text_encoder = create_text_encoder(cfg)\n",
    "    model = CLIPModel(image_encoder, text_encoder, cfg).to(cfg.device)\n",
    "    \n",
    "    model_path = config['model_save_path']\n",
    "    \n",
    "    if os.path.exists(model_path) and not cfg.force_retrain:\n",
    "        print(f\"Model found at '{model_path}'. Loading weights and skipping training.\")\n",
    "        checkpoint = torch.load(model_path, map_location=cfg.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        training_history = None # No training was performed\n",
    "    else:\n",
    "        print(f\"Model not found or force_retrain is True. Starting training...\")\n",
    "        params = [\n",
    "            {\"params\": model.image_encoder.parameters(), \"lr\": cfg.image_encoder_lr},\n",
    "            {\"params\": model.text_encoder.parameters(), \"lr\": cfg.text_encoder_lr},\n",
    "            {\"params\": list(model.image_projection.parameters()) + list(model.text_projection.parameters()), \"lr\": cfg.head_lr, \"weight_decay\": cfg.weight_decay}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=len(train_df) // cfg.batch_size * cfg.epochs, eta_min=cfg.lr_min)\n",
    "        loss_fn = CLIPLoss(temperature=cfg.temperature).to(cfg.device)\n",
    "\n",
    "        trainer = Trainer(model, optimizer, lr_scheduler, loss_fn, cfg)\n",
    "        training_history = trainer.fit(train_df, valid_loader, tokenizer, config)\n",
    "\n",
    "    if not cfg.run_embedding_generation and not cfg.run_evaluation:\n",
    "        print(\"Skipping embedding generation and evaluation as per configuration.\")\n",
    "        return training_history, {}\n",
    "\n",
    "    # --- Embedding Generation ---\n",
    "    embedding_dir = config['embedding_save_path']\n",
    "    train_embed_path = os.path.join(embedding_dir, 'train_image_embeddings.pt')\n",
    "    valid_embed_path = os.path.join(embedding_dir, 'valid_image_embeddings.pt')\n",
    "    \n",
    "    if cfg.run_embedding_generation:\n",
    "        full_train_loader = build_loaders(train_df, tokenizer, mode=\"valid\", config=config, cfg=cfg)\n",
    "        if not os.path.exists(train_embed_path):\n",
    "            print(\"\\n--- Generating Embeddings for the TRAINING set ---\")\n",
    "            generate_and_save_embeddings(model, full_train_loader, config, cfg, \"train\")\n",
    "        else:\n",
    "            print(\"\\nTraining embeddings found. Skipping generation.\")\n",
    "    \n",
    "        if not os.path.exists(valid_embed_path):\n",
    "            print(\"\\n--- Generating Embeddings for the VALIDATION set ---\")\n",
    "            generate_and_save_embeddings(model, valid_loader, config, cfg, \"valid\")\n",
    "        else:\n",
    "            print(\"Validation embeddings found. Skipping generation.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping embedding generation as per configuration.\")\n",
    "\n",
    "    # --- Retrieval Accuracy Calculation ---\n",
    "    final_metrics = {}\n",
    "    if cfg.run_evaluation:\n",
    "        print(\"\\nProceeding to retrieval accuracy calculation...\")\n",
    "        try:\n",
    "            image_embeddings = torch.load(os.path.join(embedding_dir, 'valid_image_embeddings.pt'))\n",
    "            text_embeddings = torch.load(os.path.join(embedding_dir, 'valid_text_embeddings.pt'))\n",
    "            ground_truth = torch.load(os.path.join(embedding_dir, 'valid_ground_truth.pt'))\n",
    "            \n",
    "            final_metrics = evaluate_retrieval(image_embeddings, text_embeddings, ground_truth, config, cfg)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"Could not find validation embedding files to calculate accuracy.\")\n",
    "        \n",
    "        # --- Qualitative Analysis ---\n",
    "        show_qualitative_results(config, cfg)\n",
    "    else:\n",
    "        print(\"Skipping evaluation as per configuration.\")\n",
    "    \n",
    "    print(f\"PIPELINE FOR {config['dataset_name'].upper()} COMPLETE\\n\")\n",
    "    return training_history, final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 8: Main Execution and Comparative Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_text_to_image_examples(config, cfg, num_examples=3):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           QUALITATIVE ANALYSIS: TEXT-TO-IMAGE RETRIEVAL\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    embedding_dir = config['embedding_save_path']\n",
    "    try:\n",
    "        image_embeddings = torch.load(os.path.join(embedding_dir, 'valid_image_embeddings.pt'))\n",
    "        text_embeddings = torch.load(os.path.join(embedding_dir, 'valid_text_embeddings.pt'))\n",
    "        ground_truth_map = torch.load(os.path.join(embedding_dir, 'valid_ground_truth.pt')).numpy()\n",
    "        valid_captions = torch.load(os.path.join(embedding_dir, 'valid_captions.pt'))\n",
    "        unique_image_paths = torch.load(os.path.join(embedding_dir, 'valid_unique_image_paths.pt'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find embedding files for qualitative analysis.\")\n",
    "        return\n",
    "    \n",
    "    random_indices = random.sample(range(len(valid_captions)), num_examples)\n",
    "    \n",
    "    for idx in random_indices:\n",
    "        query_caption = valid_captions[idx]\n",
    "        true_image_idx = ground_truth_map[idx]\n",
    "        true_image_path = unique_image_paths[true_image_idx]\n",
    "        \n",
    "        query_text_embedding = text_embeddings[idx].to(cfg.device).unsqueeze(0)\n",
    "        similarities = query_text_embedding @ image_embeddings.to(cfg.device).T\n",
    "        \n",
    "        top_k_indices = torch.argsort(similarities, descending=True).squeeze()[:5]\n",
    "        \n",
    "        print(f\"QUERY CAPTION: \\\"{query_caption}\\\"\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 6, figsize=(20, 4))\n",
    "        \n",
    "        axes[0].imshow(Image.open(true_image_path))\n",
    "        axes[0].set_title(\"Ground Truth\")\n",
    "        axes[0].axis(\"off\")\n",
    "        \n",
    "        for i, img_idx in enumerate(top_k_indices):\n",
    "            retrieved_path = unique_image_paths[img_idx]\n",
    "            axes[i+1].imshow(Image.open(retrieved_path))\n",
    "            is_correct = img_idx == true_image_idx\n",
    "            border_color = 'green' if is_correct else 'red'\n",
    "            axes[i+1].set_title(f\"Rank {i+1}\", color=border_color)\n",
    "            for spine in axes[i+1].spines.values():\n",
    "                spine.set_edgecolor(border_color)\n",
    "                spine.set_linewidth(4)\n",
    "            axes[i+1].set_xticks([])\n",
    "            axes[i+1].set_yticks([])\n",
    "\n",
    "        plt.show()\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def show_image_to_text_examples(config, cfg, num_examples=3):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           QUALITATIVE ANALYSIS: IMAGE-TO-TEXT RETRIEVAL\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    embedding_dir = config['embedding_save_path']\n",
    "    try:\n",
    "        image_embeddings = torch.load(os.path.join(embedding_dir, 'valid_image_embeddings.pt'))\n",
    "        text_embeddings = torch.load(os.path.join(embedding_dir, 'valid_text_embeddings.pt'))\n",
    "        ground_truth_map = torch.load(os.path.join(embedding_dir, 'valid_ground_truth.pt')).numpy()\n",
    "        valid_captions = torch.load(os.path.join(embedding_dir, 'valid_captions.pt'))\n",
    "        unique_image_paths = torch.load(os.path.join(embedding_dir, 'valid_unique_image_paths.pt'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find embedding files for qualitative analysis.\")\n",
    "        return\n",
    "\n",
    "    image_to_captions_map = defaultdict(list)\n",
    "    for i, img_idx in enumerate(ground_truth_map):\n",
    "        image_to_captions_map[img_idx].append(i)\n",
    "        \n",
    "    unique_image_indices = list(image_to_captions_map.keys())\n",
    "    random_indices = random.sample(unique_image_indices, num_examples)\n",
    "    \n",
    "    for img_idx in random_indices:\n",
    "        query_image_embedding = image_embeddings[img_idx].to(cfg.device).unsqueeze(0)\n",
    "        similarities = query_image_embedding @ text_embeddings.to(cfg.device).T\n",
    "        top_k_indices = torch.argsort(similarities, descending=True).squeeze()[:5]\n",
    "        \n",
    "        query_image_path = unique_image_paths[img_idx]\n",
    "\n",
    "        plt.imshow(Image.open(query_image_path))\n",
    "        plt.title(f\"QUERY IMAGE: {os.path.basename(query_image_path)}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"--- Ground Truth Captions ---\")\n",
    "        ground_truth_caption_indices = image_to_captions_map[img_idx]\n",
    "        for cap_idx in ground_truth_caption_indices:\n",
    "            print(f\"- {valid_captions[cap_idx]}\")\n",
    "\n",
    "        print(\"\\n--- Top 5 Retrieved Captions ---\")\n",
    "        for i, cap_idx in enumerate(top_k_indices):\n",
    "            retrieved_caption = valid_captions[cap_idx]\n",
    "            is_correct = cap_idx in image_to_captions_map[img_idx]\n",
    "            prefix = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"{prefix} Rank {i+1}: \\\"{retrieved_caption}\\\"\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def show_text_to_text_examples(config, cfg, num_examples=3):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           QUALITATIVE ANALYSIS: TEXT-TO-TEXT RETRIEVAL\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    embedding_dir = config['embedding_save_path']\n",
    "    try:\n",
    "        text_embeddings = torch.load(os.path.join(embedding_dir, 'valid_text_embeddings.pt'))\n",
    "        ground_truth_map = torch.load(os.path.join(embedding_dir, 'valid_ground_truth.pt')).numpy()\n",
    "        valid_captions = torch.load(os.path.join(embedding_dir, 'valid_captions.pt'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find embedding files for qualitative analysis.\")\n",
    "        return\n",
    "\n",
    "    image_to_captions_map = defaultdict(list)\n",
    "    for i, img_idx in enumerate(ground_truth_map):\n",
    "        image_to_captions_map[img_idx].append(i)\n",
    "\n",
    "    random_indices = random.sample(range(len(valid_captions)), num_examples)\n",
    "\n",
    "    for idx in random_indices:\n",
    "        query_caption = valid_captions[idx]\n",
    "        query_text_embedding = text_embeddings[idx].to(cfg.device).unsqueeze(0)\n",
    "        similarities = query_text_embedding @ text_embeddings.to(cfg.device).T\n",
    "        similarities[0, idx] = -torch.inf # Exclude self-retrieval\n",
    "        top_k_indices = torch.argsort(similarities, descending=True).squeeze()[:5]\n",
    "\n",
    "        print(f\"QUERY CAPTION: \\\"{query_caption}\\\"\")\n",
    "\n",
    "        ground_truth_image_id = ground_truth_map[idx]\n",
    "        print(\"--- Ground Truth Captions (from same image) ---\")\n",
    "        ground_truth_caption_indices = image_to_captions_map[ground_truth_image_id]\n",
    "        for cap_idx in ground_truth_caption_indices:\n",
    "            if cap_idx != idx: print(f\"- {valid_captions[cap_idx]}\")\n",
    "\n",
    "        print(\"\\n--- Top 5 Retrieved Captions ---\")\n",
    "        for i, cap_idx in enumerate(top_k_indices):\n",
    "            retrieved_caption = valid_captions[cap_idx]\n",
    "            is_correct = cap_idx in ground_truth_caption_indices\n",
    "            prefix = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"{prefix} Rank {i+1}: \\\"{retrieved_caption}\\\"\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def show_qualitative_results(config, cfg):\n",
    "    try:\n",
    "        show_text_to_image_examples(config, cfg)\n",
    "        show_image_to_text_examples(config, cfg)\n",
    "        show_text_to_text_examples(config, cfg)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during qualitative analysis: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name, base_path = detect_environment()\n",
    "    \n",
    "    results_history = {}\n",
    "    for exp_name, exp_params in experiment_configs.items():\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"                RUNNING EXPERIMENT: {exp_name.upper()}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        datasets_to_process = [\"flickr8k\", \"flickr30k\"]\n",
    "        for dataset_name in datasets_to_process:\n",
    "            base_cfg_dict = {k: v for k, v in BaseCFG.__dict__.items() if not k.startswith('__')}\n",
    "            combined_params = {**base_cfg_dict, **exp_params[\"models\"], **exp_params[\"hyperparameters\"][dataset_name]}\n",
    "            cfg = SimpleNamespace(**combined_params)\n",
    "            \n",
    "            path_config = generate_paths(base_path, dataset_name, cfg)\n",
    "            prepare_dataset(path_config)\n",
    "            history, metrics = run_pipeline(path_config, cfg)\n",
    "            \n",
    "            if dataset_name not in results_history:\n",
    "                results_history[dataset_name] = {}\n",
    "            results_history[dataset_name][exp_name] = {'history': history, 'metrics': metrics}\n",
    "\n",
    "    if BaseCFG.show_comparative_plots:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"           FINAL COMPARATIVE ANALYSIS\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        if results_history:\n",
    "            # --- 1. Side-by-Side Loss Plots ---\n",
    "            for exp_name in experiment_configs.keys():\n",
    "                num_datasets = len(results_history)\n",
    "                if num_datasets > 0:\n",
    "                    fig, axes = plt.subplots(1, num_datasets, figsize=(8 * num_datasets, 5), squeeze=False)\n",
    "                    fig.suptitle(f'Training & Validation Loss Comparison - {exp_name.upper()}', fontsize=16)\n",
    "\n",
    "                    dataset_names = list(results_history.keys())\n",
    "                    for i in range(num_datasets):\n",
    "                        ax = axes[0, i]\n",
    "                        dataset_name = dataset_names[i]\n",
    "                        history = results_history[dataset_name][exp_name].get('history')\n",
    "                        if history and history['train_loss']:\n",
    "                            ax.plot(range(1, len(history['train_loss']) + 1), history['train_loss'], label='Train Loss')\n",
    "                            ax.plot(range(1, len(history['valid_loss']) + 1), history['valid_loss'], label='Validation Loss')\n",
    "                            ax.set_title(dataset_name)\n",
    "                            ax.set_xlabel('Epochs')\n",
    "                            ax.set_ylabel('Loss')\n",
    "                            ax.legend()\n",
    "                            ax.grid(True)\n",
    "                        else:\n",
    "                            ax.text(0.5, 0.5, 'Training was skipped', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "                            ax.set_title(dataset_name)\n",
    "\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "                    plt.show()\n",
    "\n",
    "            # --- 2. Comparative Metrics Table ---\n",
    "            for exp_name in experiment_configs.keys():\n",
    "                display(Markdown(f\"### Results for Experiment: {exp_name.upper()}\"))\n",
    "                t2i_summary_table = \"| Dataset   | Top-1 Accuracy | Top-1 Precision | Top-1 Recall |\\n\"\n",
    "                t2i_summary_table += \"|-----------|----------------|-----------------|--------------|\\n\"\n",
    "                i2t_summary_table = \"| Dataset   | Top-1 Accuracy | Top-1 Precision | Top-1 Recall |\\n\"\n",
    "                i2t_summary_table += \"|-----------|----------------|-----------------|--------------|\\n\"\n",
    "                t2t_summary_table = \"| Dataset   | Top-1 Accuracy | Top-1 Precision | Top-1 Recall |\\n\"\n",
    "                t2t_summary_table += \"|-----------|----------------|-----------------|--------------|\\n\"\n",
    "\n",
    "                for dataset_name in datasets_to_process:\n",
    "                    metrics = results_history.get(dataset_name, {}).get(exp_name, {}).get('metrics')\n",
    "                    if metrics:\n",
    "                        t2i_metrics = metrics.get('t2i')\n",
    "                        i2t_metrics = metrics.get('i2t')\n",
    "                        t2t_metrics = metrics.get('t2t')\n",
    "                        if t2i_metrics and 1 in t2i_metrics:\n",
    "                            acc, prec, rec = t2i_metrics[1]['accuracy'], t2i_metrics[1]['precision'], t2i_metrics[1]['recall']\n",
    "                            t2i_summary_table += f\"| {dataset_name:<9} | {acc:<14.4f} | {prec:<15.4f} | {rec:<12.4f} |\\n\"\n",
    "                        if i2t_metrics and 1 in i2t_metrics:\n",
    "                            acc, prec, rec = i2t_metrics[1]['accuracy'], i2t_metrics[1]['precision'], i2t_metrics[1]['recall']\n",
    "                            i2t_summary_table += f\"| {dataset_name:<9} | {acc:<14.4f} | {prec:<15.4f} | {rec:<12.4f} |\\n\"\n",
    "                        if t2t_metrics and 1 in t2t_metrics:\n",
    "                            acc, prec, rec = t2t_metrics[1]['accuracy'], t2t_metrics[1]['precision'], t2t_metrics[1]['recall']\n",
    "                            t2t_summary_table += f\"| {dataset_name:<9} | {acc:<14.4f} | {prec:<15.4f} | {rec:<12.4f} |\\n\"\n",
    "                \n",
    "                display(Markdown(\"#### Text-to-Image Retrieval Summary\"))\n",
    "                display(Markdown(t2i_summary_table))\n",
    "                display(Markdown(\"#### Image-to-Text Retrieval Summary\"))\n",
    "                display(Markdown(i2t_summary_table))\n",
    "                display(Markdown(\"#### Text-to-Text Retrieval Summary\"))\n",
    "                display(Markdown(t2t_summary_table))\n",
    "\n",
    "        else:\n",
    "            print(\"No results were generated to compare.\")\n",
    "\n",
    "    print(\"\\nAll dataset pipelines have been executed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
